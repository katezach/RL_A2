{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM2EcA7cNo+5enHYQjxfxh2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MnR4xCB1kAls","executionInfo":{"status":"ok","timestamp":1683399658073,"user_tz":-120,"elapsed":5,"user":{"displayName":"kate zach","userId":"06619662069210292564"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"feb2ff14-3da1-4441-cb34-ab622352806c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.distributions import MultivariateNormal\n","from torch.distributions import Categorical\n","from torch import optim\n","\n","import numpy as np\n","\n","\n","class RolloutBuffer:\n","    def __init__(self):\n","        self.actions = []\n","        self.states = []\n","        self.logprobs = []\n","        self.rewards = []\n","        self.state_values = []\n","        self.dones = []\n","\n","    def clear(self):\n","        del self.actions[:]\n","        del self.states[:]\n","        del self.logprobs[:]\n","        del self.rewards[:]\n","        del self.state_values[:]\n","        del self.dones[:]\n","\n","class ActorCriticPPO(nn.Module):\n","    def __init__(self, state_size, action_size, hidden_size):\n","        super(ActorCriticPPO, self).__init__()\n","\n","        # actor\n","        self.actor = nn.Sequential(\n","                        nn.Linear(state_size, hidden_size),\n","                        nn.Tanh(),\n","                        nn.Linear(hidden_size, hidden_size),\n","                        nn.Tanh(),\n","                        nn.Linear(hidden_size, action_size),\n","                        nn.Softmax(dim=-1)\n","                    )\n","        # critic\n","        self.critic = nn.Sequential(\n","                        nn.Linear(state_size, hidden_size),\n","                        nn.Tanh(),\n","                        nn.Linear(hidden_size, hidden_size),\n","                        nn.Tanh(),\n","                        nn.Linear(hidden_size, 1)\n","                    )\n","        \n","    \n","    def forward(self):\n","        raise NotImplementedError\n","    \n","\n","    def act(self, state):\n","      action_probs = self.actor(state)\n","      dist = Categorical(action_probs)\n","\n","      action = dist.sample()\n","      action_logprob = dist.log_prob(action)\n","      state_val = self.critic(state)\n","\n","      return action.detach(), action_logprob.detach(), state_val.detach()\n","    \n","\n","    def evaluate(self, state, action):\n","        action_probs = self.actor(state)\n","        dist = Categorical(action_probs)\n","\n","        action_logprobs = dist.log_prob(action)\n","        dist_entropy = dist.entropy()\n","        state_values = self.critic(state)\n","        \n","        return action_logprobs, state_values, dist_entropy\n","\n","\n","class PPO:\n","    def __init__(self, env, obs_type, max_steps):\n","        self.env = env\n","        \n","        if obs_type == 'vector':\n","          self.state_size = env.observation_space.shape[0]\n","        elif obs_type == 'pixel':\n","          self.state_size = env.observation_space.shape\n","          self.state_size = self.state_size[0]*self.state_size[1]*self.state_size[2]\n","\n","        self.action_size = env.action_space.n\n","        self.hidden_size = 64\n","\n","        self.gamma = 0.9\n","        self.update_timestep = max_steps * 5   # update policy every n timesteps\n","        self.epochs = 40                     # update policy for epochs\n","        self.eps_clip = 0.1                    # clip parameter for PPO\n","        self.lr_actor = 0.001                  # learning rate for actor network\n","        self.lr_critic = 0.01                 # learning rate for critic network\n","\n","        self.buffer = RolloutBuffer()\n","\n","        self.policy = ActorCriticPPO(self.state_size, self.action_size, self.hidden_size)\n","        self.optimizer = torch.optim.Adam([\n","                        {'params': self.policy.actor.parameters(), 'lr': self.lr_actor},\n","                        {'params': self.policy.critic.parameters(), 'lr': self.lr_critic}\n","                    ])\n","\n","        self.policy_old = ActorCriticPPO(self.state_size, self.action_size, self.hidden_size)\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","        \n","        self.MseLoss = nn.MSELoss()\n","\n","\n","    def select_action(self, state):\n","\n","        with torch.no_grad():\n","            state = torch.FloatTensor(state)\n","            action, action_logprob, state_val = self.policy_old.act(state)\n","        \n","        self.buffer.states.append(state)\n","        self.buffer.actions.append(action)\n","        self.buffer.logprobs.append(action_logprob)\n","        self.buffer.state_values.append(state_val)\n","\n","        return action.item()\n","\n","    def compute_returns(self):\n","        returns = []\n","        R = 0\n","        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.dones)):\n","            if is_terminal:\n","                R = 0\n","            R = reward + (self.gamma * R)\n","            returns.insert(0, R)\n","            \n","        returns = torch.tensor(returns, dtype=torch.float32)\n","        returns = (returns - returns.mean()) / (returns.std() + 1e-7)\n","\n","        return returns\n","\n","    def update(self):\n","        returns = self.compute_returns()\n","        weight = 0.01\n","\n","        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach()\n","        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach()\n","        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach()\n","        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach()\n","\n","        advantages = returns.detach() - old_state_values.detach()\n","        \n","        for _ in range(self.epochs):\n","            logprobs, state_values, entropy = self.policy.evaluate(old_states, old_actions) # Evaluating old actions and values\n","            state_values = torch.squeeze(state_values)     # match state_values tensor dimensions with rewards tensor\n","            ratios = torch.exp(logprobs - old_logprobs.detach())  # Finding the ratio (pi_theta / pi_theta__old)\n","\n","            # Finding Surrogate Loss   \n","            surr1 = ratios * advantages\n","            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n","\n","            # final loss of clipped objective PPO\n","            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, returns) - weight * entropy\n","            \n","            # take gradient step\n","            self.optimizer.zero_grad()\n","            loss.mean().backward()\n","            self.optimizer.step()\n","            \n","        # Copy new weights into old policy\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","\n","        # clear buffer\n","        self.buffer.clear()\n","    \n","    def train(self, model, episodes, max_training_timesteps):\n","        episode = 0\n","        self.episode_rewards=[]\n","        time_step=0\n","        # training loop\n","        while time_step <= max_training_timesteps:\n","            \n","            state = torch.tensor(self.env.reset(), dtype=torch.float).flatten()\n","            total_rewards = 0\n","            for t in range(1, episodes+1):\n","                \n","                # select action with policy\n","                action = model.select_action(state)\n","                next_state, reward, done = env.step(action)\n","                \n","                state = torch.tensor(next_state, dtype=torch.float).flatten()\n","                \n","                model.buffer.rewards.append(reward)\n","                model.buffer.dones.append(done)\n","                \n","                \n","                total_rewards += reward\n","\n","                if done:\n","                  break\n","\n","                time_step +=1\n","                \n","                if time_step % self.update_timestep == 0:\n","                    model.update()\n","\n","                if time_step % 1000 == 0:\n","                    print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Total Reward : {}\".format(episode, time_step, total_rewards))\n","\n","            \n","            episode += 1\n","\n","\n","       "]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\n","import matplotlib\n","import torch\n","\n","# matplotlib.use('TkAgg')  # 'Qt5Agg') # 'TkAgg'\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","import numpy as np\n","from gym import spaces\n","\n","ACTION_EFFECTS = (-1, 0, 1)  # left, idle right.\n","OBSERVATION_TYPES = ['pixel', 'vector']\n","\n","\n","class Catch():\n","    \n","\n","    def __init__(self, rows: int = 7, columns: int = 7, speed: float = 1.0,\n","                 max_steps: int = 250, max_misses: int = 10,\n","                 observation_type: str = 'pixel', seed=None,\n","                 ):\n","        \"\"\" Arguments: \n","        rows: the number of rows in the environment grid.\n","        columns: number of columns in the environment grid.\n","        speed: speed of dropping new balls. At 1.0 (default), we drop a new ball whenever the last one drops from the bottom. \n","        max_steps: number of steps after which the environment terminates.\n","        max_misses: number of missed balls after which the environment terminates (when this happens before 'max_steps' is reached).\n","        observation_type: type of observation, either 'vector' or 'pixel'. \n","              - 'vector': observation is a vector of length 3:  [x_paddle,x_lowest_ball,y_lowest_ball]\n","              - 'pixel': observation is an array of size [rows x columns x 2], with one hot indicator for the paddle location in the first channel,\n","              and one-hot indicator for every present ball in the second channel. \n","        seed: environment seed. \n","        \"\"\"\n","        if observation_type not in OBSERVATION_TYPES:\n","            raise ValueError('Invalid \"observation_type\". Needs to be in  {}'.format(OBSERVATION_TYPES))\n","        if speed <= 0.0:\n","            raise ValueError('Dropping \"speed\" should be larger than 0.0')\n","\n","        # store arguments\n","        self._rng = np.random.RandomState(seed)\n","        self.rows = rows\n","        self.columns = columns\n","        self.speed = speed\n","        self.max_steps = max_steps\n","        self.max_misses = max_misses\n","        self.observation_type = observation_type\n","\n","        # compute the drop interval \n","        self.drop_interval = max(1, rows // speed)  # compute the interval towards the next drop, can never drop below 1\n","        if speed != 1.0 and observation_type == 'vector':\n","            print(\n","                'Warning: You use speed > 1.0, which means there may be multiple balls in the screen at the same time.' +\n","                'However, with observation_type = vector, only the xy location of *lowest* ball is visible to the agent' +\n","                ' (to ensure a fixed length observation vector')\n","\n","        # Initialize counter\n","        self.total_timesteps = None\n","        self.fig = None\n","        self.action_space = spaces.Discrete(3, )\n","        if self.observation_type == 'vector':\n","            self.observation_space = spaces.Box(low=np.array((0, 0, 0)),\n","                                                high=np.array((self.columns, self.columns, self.rows)), dtype=int)\n","        elif self.observation_type == 'pixel':\n","            self.observation_space = spaces.Box(low=np.zeros((self.rows, self.columns, 2)),\n","                                                high=np.ones((self.rows, self.columns, 2)), dtype=int)\n","\n","    def reset(self):\n","        ''' Reset the problem to empty board with paddle in the middle bottom and a first ball on a random location in the top row '''\n","        # reset all counters\n","        self.total_timesteps = 0\n","        self.total_reward = 0\n","        self.r = '-'\n","        self.missed_balls = 0\n","        self.time_till_next_drop = self.drop_interval\n","        self.terminal = False\n","\n","        # initialized problem\n","        self.paddle_xy = [self.columns // 2, 0]  # paddle in the bottom middle\n","        self.balls_xy = []  # empty the current balls\n","        self._drop_new_ball()  # add the first ball\n","        s0 = self._get_state()  # get first state\n","        return s0\n","\n","    def step(self, a):\n","        ''' Forward the environment one step based on provided action a '''\n","\n","        # Check whether step is even possible\n","        if self.total_timesteps is None:\n","            ValueError(\"You need to reset() the environment before you can call step()\")\n","        elif self.terminal:\n","            ValueError(\"Environment has terminated, you need to call reset() first\")\n","\n","        # Move the paddle based on the chosen action\n","        self.paddle_xy[0] = np.clip(self.paddle_xy[0] + ACTION_EFFECTS[a], 0, self.columns - 1)\n","\n","        # Drop all balls one step down\n","        for ball in self.balls_xy:\n","            ball[1] -= 1\n","\n","        # Check whether lowest ball dropped from the bottom\n","        if len(self.balls_xy) > 0:  # there is a ball present\n","            if self.balls_xy[0][1] < 0:  # the lowest ball reached below the bottom\n","                del self.balls_xy[0]\n","\n","        # Check whether we need to drop a new ball\n","        self.time_till_next_drop -= 1\n","        if self.time_till_next_drop == 0:\n","            self._drop_new_ball()\n","            self.time_till_next_drop = self.drop_interval\n","\n","            # Compute rewards\n","        if (len(self.balls_xy) == 0) or (self.balls_xy[0][1] != 0):  # no ball present at bottom row\n","            r = 0.0\n","        elif self.balls_xy[0][0] == self.paddle_xy[0]:  # ball and paddle location match, caught a ball\n","            r = 1.0\n","        else:  # missed the ball\n","            r = -1.0\n","            self.missed_balls += 1\n","\n","        # Compute termination\n","        self.total_timesteps += 1\n","        if (self.total_timesteps == self.max_steps) or (self.missed_balls == self.max_misses):\n","            self.terminal = True\n","        else:\n","            self.terminal = False\n","\n","        self.r = r\n","        self.total_reward += r\n","        return self._get_state(), r, self.terminal #, {}\n","\n","    def render(self, step_pause=0.3):\n","        ''' Render the current environment situation '''\n","        if self.total_timesteps is None:\n","            ValueError(\"You need to reset() the environment before you render it\")\n","\n","        # In first call initialize figure\n","        if self.fig == None:\n","            self._initialize_plot()\n","\n","        # Set all colors to white\n","        for x in range(self.columns):\n","            for y in range(self.rows):\n","                if self.paddle_xy == [x, y]:  # hit the agent location\n","                    if [x, y] in self.balls_xy:  # agent caught a ball\n","                        self.patches[x][y].set_color('g')\n","                    else:\n","                        self.patches[x][y].set_color('y')\n","                elif [x, y] in self.balls_xy:  # hit a ball location without agent\n","                    if y == 0:  # missed the ball\n","                        self.patches[x][y].set_color('r')\n","                    else:  # just a ball\n","                        self.patches[x][y].set_color('w')\n","                else:  # empty spot\n","                    self.patches[x][y].set_color('k')\n","        # plt.axis('off')\n","\n","        self.label.set_text(\n","            'Reward:  {:<5}            Total reward:  {:<5}     \\nTotal misses: {:>2}/{:<2}     Timestep: {:>3}/{:<3}'.format(\n","                self.r, self.total_reward, self.missed_balls, self.max_misses, self.total_timesteps, self.max_steps))\n","\n","        # Draw figure\n","        plt.pause(step_pause)\n","\n","    def _initialize_plot(self):\n","        ''' initializes the catch environment figure '''\n","        self.fig, self.ax = plt.subplots()\n","        self.fig.set_figheight(self.rows)\n","        self.fig.set_figwidth(self.columns)\n","        self.ax.set_aspect('equal', adjustable='box')\n","        self.ax.set_xlim([0, self.columns])\n","        self.ax.set_ylim([0, self.rows])\n","        self.ax.axes.xaxis.set_visible(False)\n","        self.ax.axes.yaxis.set_visible(False)\n","\n","        self.patches = [[[] for x in range(self.rows)] for y in range(self.columns)]\n","        for x in range(self.columns):\n","            for y in range(self.rows):\n","                self.patches[x][y] = Rectangle((x, y), 1, 1, linewidth=0.0, color='k')\n","                self.ax.add_patch(self.patches[x][y])\n","\n","        self.label = self.ax.text(0.01, self.rows + 0.2, '', fontsize=20, c='k')\n","\n","    def _drop_new_ball(self):\n","        ''' drops a new ball from the top '''\n","        self.balls_xy.append([self._rng.randint(self.columns), self.rows - 1])  # 0])\n","\n","    def _get_state(self):\n","        ''' Returns the current agent observation '''\n","        if self.observation_type == 'vector':\n","            if len(self.balls_xy) > 0:  # balls present\n","                s = np.append(self.paddle_xy[0], self.balls_xy[0]).astype('float32')  # paddle xy and ball xy\n","            else:\n","                s = np.append(self.paddle_xy[0], [-1, -1]).astype(\n","                    'float32')  # no balls, impute (-1,-1) in state for no ball present\n","        elif self.observation_type == 'pixel':\n","            s = np.zeros((self.columns, self.rows, 2), dtype=np.float32)\n","            s[self.paddle_xy[0], self.paddle_xy[1], 0] = 1.0  # set paddle indicator in first slice\n","            for ball in self.balls_xy:\n","                s[ball[0], ball[1], 1] = 1.0  # set ball indicator(s) in second slice\n","        else:\n","            raise ValueError('observation_type not recognized, needs to be in {}'.format(OBSERVATION_TYPES))\n","        return s\n","\n","\n","if __name__ == '__main__':\n","\n","    obs_type = 'pixel'\n","    env = Catch(rows=7, columns=7, speed=1.0, max_steps=300, max_misses=10, observation_type='pixel', seed=None)\n","   \n","    episodes = 2000\n","    max_steps = 300\n","\n","    max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n","    \n","    ppo_agent = PPO(env, obs_type, max_steps)\n","    ppo_agent.train(ppo_agent, episodes, max_training_timesteps)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVC2dRw9kE0R","executionInfo":{"status":"ok","timestamp":1683399731467,"user_tz":-120,"elapsed":71932,"user":{"displayName":"kate zach","userId":"06619662069210292564"}},"outputId":"289686d7-b86a-4b61-e9e4-f8ea473db576"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode : 11 \t\t Timestep : 1000 \t\t Total Reward : -6.0\n","Episode : 24 \t\t Timestep : 2000 \t\t Total Reward : 0.0\n","Episode : 36 \t\t Timestep : 3000 \t\t Total Reward : 0.0\n","Episode : 48 \t\t Timestep : 4000 \t\t Total Reward : -4.0\n","Episode : 60 \t\t Timestep : 5000 \t\t Total Reward : -3.0\n","Episode : 72 \t\t Timestep : 6000 \t\t Total Reward : -3.0\n","Episode : 94 \t\t Timestep : 8000 \t\t Total Reward : -1.0\n","Episode : 105 \t\t Timestep : 9000 \t\t Total Reward : -4.0\n","Episode : 116 \t\t Timestep : 10000 \t\t Total Reward : -7.0\n","Episode : 128 \t\t Timestep : 11000 \t\t Total Reward : -4.0\n","Episode : 140 \t\t Timestep : 12000 \t\t Total Reward : -2.0\n","Episode : 149 \t\t Timestep : 13000 \t\t Total Reward : -6.0\n","Episode : 160 \t\t Timestep : 14000 \t\t Total Reward : -2.0\n","Episode : 171 \t\t Timestep : 15000 \t\t Total Reward : -1.0\n","Episode : 181 \t\t Timestep : 16000 \t\t Total Reward : -7.0\n","Episode : 191 \t\t Timestep : 17000 \t\t Total Reward : -1.0\n","Episode : 202 \t\t Timestep : 18000 \t\t Total Reward : -6.0\n","Episode : 212 \t\t Timestep : 19000 \t\t Total Reward : -7.0\n","Episode : 221 \t\t Timestep : 20000 \t\t Total Reward : -2.0\n","Episode : 231 \t\t Timestep : 21000 \t\t Total Reward : 1.0\n","Episode : 239 \t\t Timestep : 22000 \t\t Total Reward : -8.0\n","Episode : 249 \t\t Timestep : 23000 \t\t Total Reward : -1.0\n","Episode : 256 \t\t Timestep : 24000 \t\t Total Reward : 1.0\n","Episode : 262 \t\t Timestep : 25000 \t\t Total Reward : -4.0\n","Episode : 270 \t\t Timestep : 26000 \t\t Total Reward : -3.0\n","Episode : 277 \t\t Timestep : 27000 \t\t Total Reward : 4.0\n","Episode : 283 \t\t Timestep : 28000 \t\t Total Reward : 10.0\n","Episode : 289 \t\t Timestep : 29000 \t\t Total Reward : 2.0\n","Episode : 295 \t\t Timestep : 30000 \t\t Total Reward : 1.0\n","Episode : 300 \t\t Timestep : 31000 \t\t Total Reward : -1.0\n","Episode : 306 \t\t Timestep : 32000 \t\t Total Reward : 10.0\n","Episode : 311 \t\t Timestep : 33000 \t\t Total Reward : 10.0\n","Episode : 315 \t\t Timestep : 34000 \t\t Total Reward : 11.0\n","Episode : 320 \t\t Timestep : 35000 \t\t Total Reward : 11.0\n","Episode : 324 \t\t Timestep : 36000 \t\t Total Reward : 1.0\n","Episode : 327 \t\t Timestep : 37000 \t\t Total Reward : 11.0\n","Episode : 331 \t\t Timestep : 38000 \t\t Total Reward : 20.0\n","Episode : 335 \t\t Timestep : 39000 \t\t Total Reward : 5.0\n","Episode : 338 \t\t Timestep : 40000 \t\t Total Reward : 7.0\n","Episode : 342 \t\t Timestep : 41000 \t\t Total Reward : 2.0\n","Episode : 345 \t\t Timestep : 42000 \t\t Total Reward : 14.0\n","Episode : 348 \t\t Timestep : 43000 \t\t Total Reward : 30.0\n","Episode : 352 \t\t Timestep : 44000 \t\t Total Reward : 6.0\n","Episode : 355 \t\t Timestep : 45000 \t\t Total Reward : 18.0\n","Episode : 358 \t\t Timestep : 46000 \t\t Total Reward : 26.0\n","Episode : 362 \t\t Timestep : 47000 \t\t Total Reward : 2.0\n","Episode : 365 \t\t Timestep : 48000 \t\t Total Reward : 18.0\n","Episode : 368 \t\t Timestep : 49000 \t\t Total Reward : 30.0\n","Episode : 372 \t\t Timestep : 50000 \t\t Total Reward : 4.0\n","Episode : 375 \t\t Timestep : 51000 \t\t Total Reward : 18.0\n","Episode : 378 \t\t Timestep : 52000 \t\t Total Reward : 32.0\n","Episode : 382 \t\t Timestep : 53000 \t\t Total Reward : 6.0\n","Episode : 385 \t\t Timestep : 54000 \t\t Total Reward : 18.0\n","Episode : 388 \t\t Timestep : 55000 \t\t Total Reward : 32.0\n","Episode : 392 \t\t Timestep : 56000 \t\t Total Reward : 6.0\n","Episode : 395 \t\t Timestep : 57000 \t\t Total Reward : 20.0\n","Episode : 398 \t\t Timestep : 58000 \t\t Total Reward : 32.0\n","Episode : 402 \t\t Timestep : 59000 \t\t Total Reward : 6.0\n","Episode : 405 \t\t Timestep : 60000 \t\t Total Reward : 20.0\n","Episode : 408 \t\t Timestep : 61000 \t\t Total Reward : 34.0\n","Episode : 412 \t\t Timestep : 62000 \t\t Total Reward : 6.0\n","Episode : 415 \t\t Timestep : 63000 \t\t Total Reward : 20.0\n","Episode : 418 \t\t Timestep : 64000 \t\t Total Reward : 32.0\n","Episode : 422 \t\t Timestep : 65000 \t\t Total Reward : 6.0\n","Episode : 425 \t\t Timestep : 66000 \t\t Total Reward : 16.0\n","Episode : 428 \t\t Timestep : 67000 \t\t Total Reward : 34.0\n","Episode : 432 \t\t Timestep : 68000 \t\t Total Reward : 6.0\n","Episode : 435 \t\t Timestep : 69000 \t\t Total Reward : 20.0\n","Episode : 438 \t\t Timestep : 70000 \t\t Total Reward : 34.0\n","Episode : 442 \t\t Timestep : 71000 \t\t Total Reward : 6.0\n","Episode : 445 \t\t Timestep : 72000 \t\t Total Reward : 18.0\n","Episode : 448 \t\t Timestep : 73000 \t\t Total Reward : 30.0\n","Episode : 452 \t\t Timestep : 74000 \t\t Total Reward : 6.0\n","Episode : 455 \t\t Timestep : 75000 \t\t Total Reward : 18.0\n","Episode : 458 \t\t Timestep : 76000 \t\t Total Reward : 32.0\n","Episode : 462 \t\t Timestep : 77000 \t\t Total Reward : 6.0\n","Episode : 465 \t\t Timestep : 78000 \t\t Total Reward : 20.0\n","Episode : 468 \t\t Timestep : 79000 \t\t Total Reward : 34.0\n","Episode : 472 \t\t Timestep : 80000 \t\t Total Reward : 6.0\n","Episode : 475 \t\t Timestep : 81000 \t\t Total Reward : 20.0\n","Episode : 478 \t\t Timestep : 82000 \t\t Total Reward : 30.0\n","Episode : 482 \t\t Timestep : 83000 \t\t Total Reward : 6.0\n","Episode : 485 \t\t Timestep : 84000 \t\t Total Reward : 20.0\n","Episode : 488 \t\t Timestep : 85000 \t\t Total Reward : 34.0\n","Episode : 492 \t\t Timestep : 86000 \t\t Total Reward : 6.0\n","Episode : 495 \t\t Timestep : 87000 \t\t Total Reward : 20.0\n","Episode : 498 \t\t Timestep : 88000 \t\t Total Reward : 32.0\n","Episode : 502 \t\t Timestep : 89000 \t\t Total Reward : 6.0\n","Episode : 505 \t\t Timestep : 90000 \t\t Total Reward : 18.0\n","Episode : 508 \t\t Timestep : 91000 \t\t Total Reward : 32.0\n","Episode : 512 \t\t Timestep : 92000 \t\t Total Reward : 6.0\n","Episode : 515 \t\t Timestep : 93000 \t\t Total Reward : 20.0\n","Episode : 518 \t\t Timestep : 94000 \t\t Total Reward : 34.0\n","Episode : 522 \t\t Timestep : 95000 \t\t Total Reward : 6.0\n","Episode : 525 \t\t Timestep : 96000 \t\t Total Reward : 18.0\n","Episode : 528 \t\t Timestep : 97000 \t\t Total Reward : 34.0\n","Episode : 532 \t\t Timestep : 98000 \t\t Total Reward : 6.0\n","Episode : 535 \t\t Timestep : 99000 \t\t Total Reward : 20.0\n","Episode : 538 \t\t Timestep : 100000 \t\t Total Reward : 34.0\n"]}]}]}