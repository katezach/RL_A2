{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sGGkFm-p3Kle"},"outputs":[],"source":["import argparse\n","import numpy as np\n","from itertools import count\n","from collections import namedtuple\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.distributions import Categorical\n","\n","from scipy.signal import savgol_filter\n","\n","\n","# parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n","# parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n","#                     help='discount factor (default: 0.99)')\n","# parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n","#                     help='interval between training status logs (default: 10)')\n","# parser.add_argument('--learning-rate', type=float, default=1e-3, metavar='LR',help='learning rate (default: 1e-3)')\n","# parser.add_argument('--filename', type=str, default='ac_agent', metavar='F')\n","# args = parser.parse_args()\n","\n","\n","\n","SaveAction = namedtuple('SavedAction', ['log_prob', 'value'])\n","\n","\n","class ActorCritic(nn.Module):\n","    def __init__(self, env, obs_type):\n","        super(ActorCritic, self).__init__()\n","        self.env = env\n","\n","        if obs_type == 'vector':\n","          self.state_size = env.observation_space.shape[0]\n","        elif obs_type == 'pixel':\n","          self.state_size = env.observation_space.shape\n","          self.state_size = self.state_size[0]*self.state_size[1]*self.state_size[2]\n","\n","        self.action_size = env.action_space.n\n","        self.hidden_size = 64\n","        \n","        self.affine1 = nn.Linear(self.state_size, self.hidden_size)\n","\n","        self.action_head = nn.Linear(self.hidden_size, self.action_size) # actor\n","        self.value_head = nn.Linear(self.hidden_size, 1) # critic\n","\n","        self.saved_actions = []\n","        self.rewards = []\n","        self.gamma = 0.99\n","        self.alpha = 0.001\n","        self.eps = np.finfo(np.float32).eps.item()\n","        self.n_steps = 5\n","\n","\n","    def forward(self, x):\n","        x = F.relu(self.affine1(x))\n","        action_prob = F.softmax(self.action_head(x), dim=-1)\n","        state_values = self.value_head(x)\n","        return action_prob, state_values\n","\n","\n","    def select_action(self, model, state):\n","        probs, state_value = model(state)\n","        m = torch.distributions.Categorical(probs)\n","        action = m.sample()\n","\n","        model.saved_actions.append(SaveAction(m.log_prob(action), state_value))\n","\n","        return action.item()\n","\n","    def compute_returns(self, model):\n","        R = 0\n","        self.saved_actions = model.saved_actions\n","        self.policy_losses = []\n","        self.value_losses = []\n","        returns = []\n","\n","        # BOOTSTRAP\n","        for t in reversed(range(len(self.saved_actions))):\n","            log_prob, value = self.saved_actions[t]\n","            reward = self.rewards[t]\n","            R = reward + self.gamma * R\n","            if t >= self.n_steps:\n","                # R -= self.gamma**self.n_steps * self.saved_actions[t-self.n_steps][1].detach().item()\n","                R += self.gamma**t * self.saved_actions[t-self.n_steps][1].detach().item()\n","            returns.insert(0, R)\n","\n","        # NOT BOOTSTRAP\n","        # for r in model.rewards[::-1]:\n","        #     R = r + self.gamma * R\n","        #     returns.insert(0, R)\n","        \n","        returns = torch.tensor(returns)\n","        returns = (returns - returns.mean()) / (returns.std() + self.eps)\n","        return returns\n","\n","    def update(self, model, optimizer):\n","        self.returns = self.compute_returns(model)\n","        weight = 0.1\n","        self.saved_actions = model.saved_actions\n","        self.policy_losses = []\n","        self.value_losses = []\n","        for (log_prob, value), R in zip(self.saved_actions, self.returns):\n","            # BASELINE SUBTRACTION\n","            # advantage = R - value.item()\n","            # m = torch.exp(log_prob)\n","            # entropy = -torch.sum(m * log_prob)\n","\n","            # self.policy_losses.append(-log_prob * advantage + weight * entropy)\n","            # self.value_losses.append(F.mse_loss(value, torch.tensor([R])))\n","\n","            # NOT BASELINE SUBTRACTION\n","            m = torch.exp(log_prob)\n","            entropy = -torch.sum(m * log_prob)\n","            policy_loss = -log_prob * R + weight * entropy\n","\n","            self.policy_losses.append(policy_loss)\n","            self.value_losses.append(F.mse_loss(value, torch.tensor([float(R)])))\n","\n","        optimizer.zero_grad()\n","        loss = torch.stack(self.policy_losses).sum() + torch.stack(self.value_losses).sum()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        del model.rewards[:]\n","        del model.saved_actions[:]\n","\n","\n","    def train(self, model, episodes):\n","      \n","      # print(f'Settings are:\\nFilename-{args.filename}\\nLR-{args.learning_rate}\\nGamma-{args.gamma}\\n')\n","      optimizer = optim.Adam(model.parameters(), lr=self.alpha)\n","      episode = 0\n","      self.episode_rewards=[]\n","\n","      while episode <= episodes:\n","          state = torch.tensor(self.env.reset(), dtype=torch.float).flatten()\n","          total_rewards = 0\n","          done = False\n","          t = 0\n","          while not done:\n","              action = self.select_action(model, state)\n","              state, reward, done = env.step(action)\n","              state = torch.tensor(state, dtype=torch.float).flatten()\n","\n","              model.rewards.append(reward)   \n","              total_rewards += reward\n","\n","\n","          model.update(model, optimizer)\n","\n","          if episode % 100 == 0:\n","              print('Episode {}\\t Total reward: {:.2f}'.format(episode, total_rewards))\n","          self.episode_rewards.append(total_rewards)\n","      \n","          episode += 1\n","  \n","      self.episode_rewards = savgol_filter(self.episode_rewards, window_length = 50, polyorder=2 )\n","      plt.plot(self.episode_rewards)\n","      plt.xlabel(\"Episode\")\n","      plt.ylabel(\"Reward\")\n","      plt.title(\"AC - Bootstrapping rewards over Episodes\")\n","      plt.show()\n","\n","      # np.save(args.filename, np.array(rep_rewards))\n","        "]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\n","import matplotlib\n","import torch\n","\n","# matplotlib.use('TkAgg')  # 'Qt5Agg') # 'TkAgg'\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","import numpy as np\n","from gym import spaces\n","\n","ACTION_EFFECTS = (-1, 0, 1)  # left, idle right.\n","OBSERVATION_TYPES = ['pixel', 'vector']\n","\n","\n","class Catch():\n","    \n","\n","    def __init__(self, rows: int = 7, columns: int = 7, speed: float = 1.0,\n","                 max_steps: int = 250, max_misses: int = 10,\n","                 observation_type: str = 'pixel', seed=None,\n","                 ):\n","        \"\"\" Arguments: \n","        rows: the number of rows in the environment grid.\n","        columns: number of columns in the environment grid.\n","        speed: speed of dropping new balls. At 1.0 (default), we drop a new ball whenever the last one drops from the bottom. \n","        max_steps: number of steps after which the environment terminates.\n","        max_misses: number of missed balls after which the environment terminates (when this happens before 'max_steps' is reached).\n","        observation_type: type of observation, either 'vector' or 'pixel'. \n","              - 'vector': observation is a vector of length 3:  [x_paddle,x_lowest_ball,y_lowest_ball]\n","              - 'pixel': observation is an array of size [rows x columns x 2], with one hot indicator for the paddle location in the first channel,\n","              and one-hot indicator for every present ball in the second channel. \n","        seed: environment seed. \n","        \"\"\"\n","        if observation_type not in OBSERVATION_TYPES:\n","            raise ValueError('Invalid \"observation_type\". Needs to be in  {}'.format(OBSERVATION_TYPES))\n","        if speed <= 0.0:\n","            raise ValueError('Dropping \"speed\" should be larger than 0.0')\n","\n","        # store arguments\n","        self._rng = np.random.RandomState(seed)\n","        self.rows = rows\n","        self.columns = columns\n","        self.speed = speed\n","        self.max_steps = max_steps\n","        self.max_misses = max_misses\n","        self.observation_type = observation_type\n","\n","        # compute the drop interval \n","        self.drop_interval = max(1, rows // speed)  # compute the interval towards the next drop, can never drop below 1\n","        if speed != 1.0 and observation_type == 'vector':\n","            print(\n","                'Warning: You use speed > 1.0, which means there may be multiple balls in the screen at the same time.' +\n","                'However, with observation_type = vector, only the xy location of *lowest* ball is visible to the agent' +\n","                ' (to ensure a fixed length observation vector')\n","\n","        # Initialize counter\n","        self.total_timesteps = None\n","        self.fig = None\n","        self.action_space = spaces.Discrete(3, )\n","        if self.observation_type == 'vector':\n","            self.observation_space = spaces.Box(low=np.array((0, 0, 0)),\n","                                                high=np.array((self.columns, self.columns, self.rows)), dtype=int)\n","        elif self.observation_type == 'pixel':\n","            self.observation_space = spaces.Box(low=np.zeros((self.rows, self.columns, 2)),\n","                                                high=np.ones((self.rows, self.columns, 2)), dtype=int)\n","\n","    def reset(self):\n","        ''' Reset the problem to empty board with paddle in the middle bottom and a first ball on a random location in the top row '''\n","        # reset all counters\n","        self.total_timesteps = 0\n","        self.total_reward = 0\n","        self.r = '-'\n","        self.missed_balls = 0\n","        self.time_till_next_drop = self.drop_interval\n","        self.terminal = False\n","\n","        # initialized problem\n","        self.paddle_xy = [self.columns // 2, 0]  # paddle in the bottom middle\n","        self.balls_xy = []  # empty the current balls\n","        self._drop_new_ball()  # add the first ball\n","        s0 = self._get_state()  # get first state\n","        return s0\n","\n","    def step(self, a):\n","        ''' Forward the environment one step based on provided action a '''\n","\n","        # Check whether step is even possible\n","        if self.total_timesteps is None:\n","            ValueError(\"You need to reset() the environment before you can call step()\")\n","        elif self.terminal:\n","            ValueError(\"Environment has terminated, you need to call reset() first\")\n","\n","        # Move the paddle based on the chosen action\n","        self.paddle_xy[0] = np.clip(self.paddle_xy[0] + ACTION_EFFECTS[a], 0, self.columns - 1)\n","\n","        # Drop all balls one step down\n","        for ball in self.balls_xy:\n","            ball[1] -= 1\n","\n","        # Check whether lowest ball dropped from the bottom\n","        if len(self.balls_xy) > 0:  # there is a ball present\n","            if self.balls_xy[0][1] < 0:  # the lowest ball reached below the bottom\n","                del self.balls_xy[0]\n","\n","        # Check whether we need to drop a new ball\n","        self.time_till_next_drop -= 1\n","        if self.time_till_next_drop == 0:\n","            self._drop_new_ball()\n","            self.time_till_next_drop = self.drop_interval\n","\n","            # Compute rewards\n","        if (len(self.balls_xy) == 0) or (self.balls_xy[0][1] != 0):  # no ball present at bottom row\n","            r = 0.0\n","        elif self.balls_xy[0][0] == self.paddle_xy[0]:  # ball and paddle location match, caught a ball\n","            r = 1.0\n","        else:  # missed the ball\n","            r = -1.0\n","            self.missed_balls += 1\n","\n","        # Compute termination\n","        self.total_timesteps += 1\n","        if (self.total_timesteps == self.max_steps) or (self.missed_balls == self.max_misses):\n","            self.terminal = True\n","        else:\n","            self.terminal = False\n","\n","        self.r = r\n","        self.total_reward += r\n","        return self._get_state(), r, self.terminal #, {}\n","\n","    def render(self, step_pause=0.3):\n","        ''' Render the current environment situation '''\n","        if self.total_timesteps is None:\n","            ValueError(\"You need to reset() the environment before you render it\")\n","\n","        # In first call initialize figure\n","        if self.fig == None:\n","            self._initialize_plot()\n","\n","        # Set all colors to white\n","        for x in range(self.columns):\n","            for y in range(self.rows):\n","                if self.paddle_xy == [x, y]:  # hit the agent location\n","                    if [x, y] in self.balls_xy:  # agent caught a ball\n","                        self.patches[x][y].set_color('g')\n","                    else:\n","                        self.patches[x][y].set_color('y')\n","                elif [x, y] in self.balls_xy:  # hit a ball location without agent\n","                    if y == 0:  # missed the ball\n","                        self.patches[x][y].set_color('r')\n","                    else:  # just a ball\n","                        self.patches[x][y].set_color('w')\n","                else:  # empty spot\n","                    self.patches[x][y].set_color('k')\n","        # plt.axis('off')\n","\n","        self.label.set_text(\n","            'Reward:  {:<5}            Total reward:  {:<5}     \\nTotal misses: {:>2}/{:<2}     Timestep: {:>3}/{:<3}'.format(\n","                self.r, self.total_reward, self.missed_balls, self.max_misses, self.total_timesteps, self.max_steps))\n","\n","        # Draw figure\n","        plt.pause(step_pause)\n","\n","    def _initialize_plot(self):\n","        ''' initializes the catch environment figure '''\n","        self.fig, self.ax = plt.subplots()\n","        self.fig.set_figheight(self.rows)\n","        self.fig.set_figwidth(self.columns)\n","        self.ax.set_aspect('equal', adjustable='box')\n","        self.ax.set_xlim([0, self.columns])\n","        self.ax.set_ylim([0, self.rows])\n","        self.ax.axes.xaxis.set_visible(False)\n","        self.ax.axes.yaxis.set_visible(False)\n","\n","        self.patches = [[[] for x in range(self.rows)] for y in range(self.columns)]\n","        for x in range(self.columns):\n","            for y in range(self.rows):\n","                self.patches[x][y] = Rectangle((x, y), 1, 1, linewidth=0.0, color='k')\n","                self.ax.add_patch(self.patches[x][y])\n","\n","        self.label = self.ax.text(0.01, self.rows + 0.2, '', fontsize=20, c='k')\n","\n","    def _drop_new_ball(self):\n","        ''' drops a new ball from the top '''\n","        self.balls_xy.append([self._rng.randint(self.columns), self.rows - 1])  # 0])\n","\n","    def _get_state(self):\n","        ''' Returns the current agent observation '''\n","        if self.observation_type == 'vector':\n","            if len(self.balls_xy) > 0:  # balls present\n","                s = np.append(self.paddle_xy[0], self.balls_xy[0]).astype('float32')  # paddle xy and ball xy\n","            else:\n","                s = np.append(self.paddle_xy[0], [-1, -1]).astype(\n","                    'float32')  # no balls, impute (-1,-1) in state for no ball present\n","        elif self.observation_type == 'pixel':\n","            s = np.zeros((self.columns, self.rows, 2), dtype=np.float32)\n","            s[self.paddle_xy[0], self.paddle_xy[1], 0] = 1.0  # set paddle indicator in first slice\n","            for ball in self.balls_xy:\n","                s[ball[0], ball[1], 1] = 1.0  # set ball indicator(s) in second slice\n","        else:\n","            raise ValueError('observation_type not recognized, needs to be in {}'.format(OBSERVATION_TYPES))\n","        return s\n","\n","\n","if __name__ == '__main__':\n","\n","    obs_type = 'pixel'\n","    env = Catch(rows=7, columns=7, speed=1.0, max_steps=250, max_misses=10, observation_type=obs_type, seed=None)\n","   \n","    episodes = 2000\n","\n","    agentAC_BS = ActorCritic(env, obs_type)\n","    agentAC_BS.train(agentAC_BS, episodes)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":696},"id":"hqiORtNq3MI1","executionInfo":{"status":"error","timestamp":1683332794467,"user_tz":-120,"elapsed":196512,"user":{"displayName":"Dimitris Kourntidis","userId":"16943485380891901348"}},"outputId":"10171774-fae4-45b3-eb5a-c48257290691"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 0\t Total reward: -8.00\n","Episode 100\t Total reward: -7.00\n","Episode 200\t Total reward: -8.00\n","Episode 300\t Total reward: -8.00\n","Episode 400\t Total reward: -7.00\n","Episode 500\t Total reward: -10.00\n","Episode 600\t Total reward: -10.00\n","Episode 700\t Total reward: -8.00\n","Episode 800\t Total reward: -8.00\n","Episode 900\t Total reward: -8.00\n","Episode 1000\t Total reward: -6.00\n","Episode 1100\t Total reward: -5.00\n","Episode 1200\t Total reward: -5.00\n","Episode 1300\t Total reward: -10.00\n","Episode 1400\t Total reward: -7.00\n","Episode 1500\t Total reward: -7.00\n","Episode 1600\t Total reward: -9.00\n","Episode 1700\t Total reward: -6.00\n","Episode 1800\t Total reward: -8.00\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-ade0ebefd240>\u001b[0m in \u001b[0;36m<cell line: 207>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0magentAC_BS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActorCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0magentAC_BS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magentAC_BS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-9054ca0d7da2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, episodes)\u001b[0m\n\u001b[1;32m    142\u001b[0m           \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m           \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m               \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m               \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m               \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-9054ca0d7da2>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, model, state)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-9054ca0d7da2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mstate_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}